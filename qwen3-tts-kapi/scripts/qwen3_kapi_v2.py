#!/usr/bin/env python3
"""
Qwen3-TTS Voice Cloning Tool (kapi2800 + bf16) v2.0

åŸºæ–¼ kapi2800/qwen3-tts-apple-silicon é …ç›®ï¼Œä½¿ç”¨ bf16 æ¨¡å‹é¿å…éœéŸ³ Bugã€‚
æ–°å¢ï¼šæƒ…ç·’æ§åˆ¶ã€å¤šç¨®é è¨­èªéŸ³ã€ä¾¿æ·çš„ Shell èª¿ç”¨æ¥å£

ç”¨æ³•:
    # åŸºç¤ç”¨æ³•
    python3 qwen3_kapi_bf16.py --text "ä½ å¥½" --voice izumi
    
    # å¸¶æƒ…ç·’
    python3 qwen3_kapi_bf16.py --text "å¤ªå¥½äº†ï¼" --voice izumi --emotion happy
    
    # è‡ªå®šç¾©è¼¸å‡º
    python3 qwen3_kapi_bf16.py --text "ä¸»äººæ™šå®‰" --voice izumi --output ~/goodnight.wav --emotion gentle

ä½œè€…: é›·å§†
æ—¥æœŸ: 2026-02-25
ç‰ˆæœ¬: 2.0
"""

import os
import sys
import time
import argparse
from pathlib import Path
from typing import Optional

# é è¨­é…ç½®
DEFAULT_OUTPUT_DIR = os.path.expanduser("~/.openclaw/tts_output")
DEFAULT_VOICES_DIR = os.path.expanduser("~/.openclaw/references")
DEFAULT_MODEL = "mlx-community/Qwen3-TTS-12Hz-1.7B-Base-bf16"

# æƒ…ç·’æ¨¡æ¿ - ç”¨æ–¼èª¿æ•´èªæ°£å’Œåƒè€ƒæ–‡æœ¬
EMOTION_PROMPTS = {
    "normal": {
        "prefix": "",
        "suffix": "",
        "description": "æ­£å¸¸èªæ°£"
    },
    "happy": {
        "prefix": "",
        "suffix": "ï¼ˆé–‹å¿ƒåœ°ï¼‰",
        "description": "é–‹å¿ƒã€èˆˆå¥®çš„èªæ°£"
    },
    "gentle": {
        "prefix": "",
        "suffix": "ï¼ˆæº«æŸ”åœ°ï¼‰",
        "description": "æº«æŸ”ã€æŸ”å’Œçš„èªæ°£"
    },
    "sad": {
        "prefix": "",
        "suffix": "ï¼ˆé›£éåœ°ï¼‰",
        "description": "æ‚²å‚·ã€é›£éçš„èªæ°£"
    },
    "angry": {
        "prefix": "",
        "suffix": "ï¼ˆç”Ÿæ°£åœ°ï¼‰",
        "description": "ç”Ÿæ°£ã€æ†¤æ€’çš„èªæ°£"
    },
    "surprised": {
        "prefix": "",
        "suffix": "ï¼ˆé©šè¨åœ°ï¼‰",
        "description": "é©šè¨ã€éœ‡é©šçš„èªæ°£"
    },
    "shy": {
        "prefix": "",
        "suffix": "ï¼ˆå®³ç¾åœ°ï¼‰",
        "description": "å®³ç¾ã€é¦è…†çš„èªæ°£"
    },
    "teasing": {
        "prefix": "",
        "suffix": "ï¼ˆèª¿çš®åœ°ï¼‰",
        "description": "èª¿çš®ã€æ‰å¼„äººçš„èªæ°£"
    }
}

# é è¨­è²éŸ³é…ç½®
PRESET_VOICES = {
    "rem": {
        "name": "é›·å§† (Rem)",
        "audio": "rem/rem_reference.wav",
        "text": "ã“ã“ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚1ã‹ã‚‰â€¦ã„ã„ãˆã€ã‚¼ãƒ­ã‹ã‚‰",
        "description": "Re:Zero é›·å§†è§’è‰²è²éŸ³ï¼Œæ—¥ç³»å¥³åƒ•é¢¨æ ¼",
        "emotions": {
            "normal": "ã“ã“ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚1ã‹ã‚‰â€¦ã„ã„ãˆã€ã‚¼ãƒ­ã‹ã‚‰",
            "happy": "ã‚¹ãƒãƒ«ãã‚“ã€ãŠã‹ãˆã‚Šãªã•ã„ï¼",
            "gentle": "ã‚¹ãƒãƒ«ãã‚“ã®ã“ã¨ã€ä¿¡ã˜ã¦ã¾ã™",
            "sad": "ã‚¹ãƒãƒ«ãã‚“â€¦ã©ã†ã—ã¦â€¦",
            "determined": "é›·ãƒ ã¯ã€ã‚¹ãƒãƒ«ãã‚“ã®ãŸã‚ã«é ‘å¼µã‚Šã¾ã™"
        }
    },
    "roxy": {
        "name": "æ´›çªå¸ŒÂ·ç±³æ ¼è·¯è¿ªäº (Roxy Migurdia)",
        "audio": "roxy/reference.wav",
        "text": "ã¯ã„ãã†ã§ã™ã­ãƒ«ãƒ‡ã‚£èº«é•·å¤§ãããªã‚Šã¾ã—ãŸã­",
        "description": "ç„¡è·è½‰ç”Ÿ æ´›çªå¸Œè§’è‰²è²éŸ³ï¼Œæ°´è–ç´šé­”è¡“å¸«ï¼Œè—é«®å¸«å‚…",
        "emotions": {
            "normal": "ã¯ã„ãã†ã§ã™ã­ãƒ«ãƒ‡ã‚£èº«é•·å¤§ãããªã‚Šã¾ã—ãŸã­",
            "happy": "ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ã“ã‚Œã§ã‚ãªãŸã¯å½—æ˜Ÿç´šé­”è¡“å¸«ã§ã™",
            "gentle": "ã™ãã«æ€–ããªããªã‚Šã¾ã™ã‚ˆã€‚ã§ã™ã‚ˆã€‚ç§ãŒã¤ã„ã¦ã„ã¾ã™ã‹ã‚‰å®‰å¿ƒã—ã¦ãã ã•ã„",
            "sad": "æ®‹å¿µã§ã™ã€‚ã“ã‚Œã§æœ¬å½“ã«ç§ãŒæ•™ãˆã‚‰ã‚Œã‚‹ã“ã¨ã‚‚ãªããªã£ã¦ã—ã¾ã„ã¾ã—ãŸ",
            "shy": "ãˆã£ã¨ã€ãƒ«ãƒ¼ãƒ‡ã‚ªã‚¹ã•ã‚“ã€ãã®ã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
            "teasing": "ã¯ã¯ãƒ¼ã‚“ã€ã•ã¦ã¯æ€–ã„ã‚“ã§ã™ã­ãƒ¼",
            "proud": "ã¡ã£ã¡ã‚ƒãã‚ã‚Šã¾ã›ã‚“ã‚ã‚Œã¯ç§ã®é«ªã‚’è¦‹ã¦é©šã„ã¦ãŸã‚“ã§ã™ã‚ˆ",
            "worried": "ãã†ã§ã™ã­è½ã¡è¾¼ã‚“ã§ã„ã‚‹ã¨ã„ã†ã®ã¯å°‘ã—å¿ƒé…ã§ã™ãŒ"
        }
    },
    "izumi": {
        "name": "å’Œæ³‰å¦ƒæ„› (Izumi Hiyori)",
        "audio": "izumi_hiyori/reference.wav",
        "text": "ã„ã‚„ã‚ã£ã¡ã‚ƒæŒã¡ã‚ã’ã‚‹ã‘ã©ã‚‚ã€æ™®æ®µé€šã‚Šã§ã„ã„ã‚ˆæ™®æ®µé€šã‚Šã§ã€‚ç§ã¨è©±ã™ã¨ãã¿ãŸã„ã«",
        "description": "å’Œæ³‰å¦ƒæ„›è§’è‰²è²éŸ³ï¼Œæ´»æ½‘å¯æ„›çš„å­¸å¦¹é¢¨æ ¼ï¼Œå¸¶æœ‰ã²ã‚ˆã²ã‚ˆå£é ­ç¦ª",
        "emotions": {
            "normal": "ã„ã‚„ã‚ã£ã¡ã‚ƒæŒã¡ã‚ã’ã‚‹ã‘ã©ã‚‚ã€æ™®æ®µé€šã‚Šã§ã„ã„ã‚ˆæ™®æ®µé€šã‚Šã§ã€‚ç§ã¨è©±ã™ã¨ãã¿ãŸã„ã«",
            "happy": "ã²ã‚ˆã²ã‚ˆï½ï¼ä»Šæ—¥ã‚‚ã„ã„å¤©æ°—ã ã­ï½",
            "gentle": "ä¸»äººã€ãŠç–²ã‚Œæ§˜ã€‚ãŠèŒ¶æ·¹ã‚Œã¦ã‚ã’ã‚‹ã­",
            "sad": "ãã‚“ãªâ€¦ã²ã‚ˆã²ã‚ˆâ€¦",
            "teasing": "ã¸ã¸ï½ã€ä¸»äººã£ãŸã‚‰ç…§ã‚Œã¦ã‚‹ï¼Ÿã²ã‚ˆã²ã‚ˆï½",
            "surprised": "ãˆã£ï¼ï¼Ÿãƒã‚¸ã§ï¼ï¼Ÿã²ã‚ˆã²ã‚ˆï¼ï¼Ÿ",
            "shy": "ã‚ã€ã‚ã®â€¦ãã®â€¦ã²ã‚ˆã²ã‚ˆâ€¦",
            "excited": "ã‚ãï½ï¼ã™ã£ã”ã„ï¼ã²ã‚ˆã²ã‚ˆï½ï¼"
        }
    }
}


def find_voice_file(audio_path: str) -> str:
    """
    æŸ¥æ‰¾è²éŸ³æ–‡ä»¶ï¼Œæ”¯æŒå¤šå€‹è·¯å¾‘ï¼š
    1. ~/.openclaw/references/
    2. è…³æœ¬æ‰€åœ¨ç›®éŒ„çš„ references/
    3. ç•¶å‰å·¥ä½œç›®éŒ„çš„ references/
    """
    possible_paths = [
        os.path.join(DEFAULT_VOICES_DIR, audio_path),
        os.path.join(os.path.dirname(__file__), "references", audio_path),
        os.path.join(os.path.dirname(os.path.abspath(__file__)), "references", audio_path),
        os.path.join(os.getcwd(), "references", audio_path),
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            return path
    
    return possible_paths[0]


def check_model_downloaded(model_id: str) -> bool:
    """æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è¼‰"""
    from huggingface_hub import try_to_load_from_cache
    
    try:
        result = try_to_load_from_cache(model_id, "config.json")
        return result is not None and str(result) != "_CACHED_NO_EXIST_"
    except:
        return False


def download_model(model_id: str):
    """ä¸‹è¼‰æ¨¡å‹"""
    from huggingface_hub import snapshot_download
    
    print(f"ğŸ“¥ ä¸‹è¼‰æ¨¡å‹: {model_id}")
    print("é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜ï¼ˆç´„ 4GBï¼‰...")
    
    snapshot_download(repo_id=model_id, local_files_only=False)
    print("âœ… æ¨¡å‹ä¸‹è¼‰å®Œæˆ")


def get_emotion_text(voice_key: str, emotion: str, base_text: str) -> str:
    """
    æ ¹æ“šæƒ…ç·’å’Œè²éŸ³ç²å–å°æ‡‰çš„åƒè€ƒæ–‡æœ¬
    
    Args:
        voice_key: è²éŸ³åç¨± (rem, izumi)
        emotion: æƒ…ç·’åç¨±
        base_text: åŸºç¤æ–‡æœ¬ï¼ˆç„¡æƒ…ç·’åŒ¹é…æ™‚ä½¿ç”¨ï¼‰
    
    Returns:
        å°æ‡‰æƒ…ç·’çš„åƒè€ƒæ–‡æœ¬
    """
    if voice_key not in PRESET_VOICES:
        return base_text
    
    voice_info = PRESET_VOICES[voice_key]
    emotions = voice_info.get("emotions", {})
    
    # å¦‚æœè©²è²éŸ³æœ‰å°æ‡‰æƒ…ç·’çš„åƒè€ƒæ–‡æœ¬ï¼Œä½¿ç”¨å®ƒ
    if emotion in emotions:
        return emotions[emotion]
    
    # å¦å‰‡ä½¿ç”¨åŸºç¤æ–‡æœ¬
    return base_text


def generate_voice(
    text: str,
    ref_audio: str,
    ref_text: str,
    output_path: str = None,
    model_id: str = DEFAULT_MODEL,
    emotion: str = "normal",
    verbose: bool = True
) -> str:
    """
    ç”ŸæˆèªéŸ³
    
    Args:
        text: è¦åˆæˆçš„æ–‡æœ¬
        ref_audio: åƒè€ƒéŸ³é »è·¯å¾‘
        ref_text: åƒè€ƒéŸ³é »å°æ‡‰çš„æ–‡æœ¬
        output_path: è¼¸å‡ºè·¯å¾‘ï¼ˆå¯é¸ï¼‰
        model_id: æ¨¡å‹ ID
        emotion: æƒ…ç·’æ¨™ç±¤
        verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°ä¿¡æ¯
    
    Returns:
        ç”Ÿæˆçš„éŸ³é »æ–‡ä»¶è·¯å¾‘
    """
    from mlx_audio.tts.utils import load_model
    from mlx_audio.tts.generate import generate_audio
    
    # æª¢æŸ¥åƒè€ƒéŸ³é »
    if not os.path.exists(ref_audio):
        raise FileNotFoundError(f"åƒè€ƒéŸ³é »ä¸å­˜åœ¨: {ref_audio}")
    
    # æª¢æŸ¥ä¸¦ä¸‹è¼‰æ¨¡å‹
    if not check_model_downloaded(model_id):
        if verbose:
            print(f"ğŸ” æ¨¡å‹æœªä¸‹è¼‰ï¼Œé–‹å§‹ä¸‹è¼‰...")
        download_model(model_id)
    
    # è™•ç†æƒ…ç·’æ¨™è¨˜
    emotion_info = EMOTION_PROMPTS.get(emotion, EMOTION_PROMPTS["normal"])
    if emotion != "normal" and emotion_info["suffix"]:
        # åœ¨æ–‡æœ¬æœ«å°¾æ·»åŠ æƒ…ç·’æ¨™è¨˜ï¼ˆå°æ¨¡å‹æœ‰æç¤ºä½œç”¨ï¼‰
        display_text = f"{text} {emotion_info['suffix']}"
    else:
        display_text = text
    
    if verbose:
        print(f"ğŸ™ï¸ Qwen3-TTS Voice Cloning")
        print(f"=" * 60)
        print(f"ğŸ“ æ–‡æœ¬: {text}")
        if emotion != "normal":
            print(f"ğŸ˜Š æƒ…ç·’: {emotion} ({emotion_info['description']})")
        print(f"ğŸ”Š åƒè€ƒ: {ref_audio}")
        print(f"ğŸ¤– æ¨¡å‹: {model_id}")
        print()
        print("ğŸ“¦ è¼‰å…¥æ¨¡å‹...")
    
    # è¼‰å…¥æ¨¡å‹
    model = load_model(model_id)
    
    if verbose:
        print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ")
        print()
        print("ğŸ™ï¸ ç”Ÿæˆä¸­...")
    
    # æº–å‚™è¼¸å‡º
    if output_path is None:
        os.makedirs(DEFAULT_OUTPUT_DIR, exist_ok=True)
        # åŒ…å«æƒ…ç·’ä¿¡æ¯çš„æ–‡ä»¶å
        if emotion != "normal":
            output_prefix = os.path.join(DEFAULT_OUTPUT_DIR, f"qwen3_{emotion}_{int(time.time())}")
        else:
            output_prefix = os.path.join(DEFAULT_OUTPUT_DIR, f"qwen3_{int(time.time())}")
    else:
        output_prefix = output_path.replace(".wav", "")
    
    # ç”Ÿæˆ
    start_time = time.time()
    generate_audio(
        model=model,
        text=text,
        ref_audio=ref_audio,
        ref_text=ref_text,
        file_prefix=output_prefix,
        audio_format="wav"
    )
    elapsed = time.time() - start_time
    
    output_file = f"{output_prefix}_000.wav"
    
    if verbose:
        print()
        print(f"âœ… ç”Ÿæˆå®Œæˆ!")
        print(f"â±ï¸ è€—æ™‚: {elapsed:.1f}ç§’")
        print(f"ğŸµ è¼¸å‡º: {output_file}")
    
    return output_file


def main():
    parser = argparse.ArgumentParser(
        description="Qwen3-TTS èªéŸ³å…‹éš†å·¥å…· v2.0 (kapi2800 + bf16 + æƒ…ç·’)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºç¤ç”¨æ³•
  %(prog)s --text "ä½ å¥½" --voice izumi
  
  # å¸¶æƒ…ç·’
  %(prog)s --text "å¤ªå¥½äº†ï¼" --voice izumi --emotion happy
  
  # è‡ªå®šç¾©è¼¸å‡ºè·¯å¾‘
  %(prog)s --text "æ™šå®‰" --voice rem --output ~/goodnight.wav --emotion gentle
  
  # åˆ—å‡ºæ‰€æœ‰è²éŸ³å’Œæƒ…ç·’
  %(prog)s --list-voices
  %(prog)s --list-emotions
        """
    )
    
    parser.add_argument("--text", "-t", help="è¦åˆæˆçš„æ–‡æœ¬")
    parser.add_argument("--ref_audio", "-a", help="åƒè€ƒéŸ³é »è·¯å¾‘")
    parser.add_argument("--ref_text", "-r", help="åƒè€ƒéŸ³é »å°æ‡‰çš„æ–‡æœ¬")
    parser.add_argument("--voice", "-v", help=f"ä½¿ç”¨é è¨­è²éŸ³: {', '.join(PRESET_VOICES.keys())}")
    parser.add_argument("--emotion", "-e", default="normal", 
                       help=f"æƒ…ç·’é¢¨æ ¼ï¼ˆé»˜èª: normalï¼‰")
    parser.add_argument("--output", "-o", help="è¼¸å‡ºæ–‡ä»¶è·¯å¾‘ï¼ˆå¯é¸ï¼‰")
    parser.add_argument("--model", "-m", default=DEFAULT_MODEL, help=f"æ¨¡å‹IDï¼ˆé»˜èª: {DEFAULT_MODEL}ï¼‰")
    parser.add_argument("--list-voices", action="store_true", help="åˆ—å‡ºæ‰€æœ‰é è¨­è²éŸ³")
    parser.add_argument("--list-emotions", action="store_true", help="åˆ—å‡ºæ‰€æœ‰æƒ…ç·’é¸é …")
    parser.add_argument("--quiet", "-q", action="store_true", help="å®‰éœæ¨¡å¼ï¼ˆåªè¼¸å‡ºæ–‡ä»¶è·¯å¾‘ï¼‰")
    
    args = parser.parse_args()
    
    # åˆ—å‡ºé è¨­è²éŸ³
    if args.list_voices:
        print("ğŸ­ å¯ç”¨é è¨­è²éŸ³:")
        for key, info in PRESET_VOICES.items():
            print(f"\n  {key}: {info['name']}")
            print(f"    {info['description']}")
            if "emotions" in info:
                print(f"    æ”¯æŒæƒ…ç·’: {', '.join(info['emotions'].keys())}")
        return
    
    # åˆ—å‡ºæƒ…ç·’é¸é …
    if args.list_emotions:
        print("ğŸ˜Š å¯ç”¨æƒ…ç·’é¸é …:")
        for key, info in EMOTION_PROMPTS.items():
            print(f"  {key}: {info['description']}")
        return
    
    # æª¢æŸ¥å¿…é ˆçš„ text åƒæ•¸ï¼ˆå¦‚æœä¸æ˜¯åœ¨åˆ—å‡ºé¸é …ï¼‰
    if not args.text:
        print("âŒ è«‹æä¾› --text åƒæ•¸ï¼Œæˆ–ä½¿ç”¨ --list-voices / --list-emotions")
        parser.print_help()
        sys.exit(1)
    
    # é©—è­‰æƒ…ç·’
    if args.emotion not in EMOTION_PROMPTS:
        print(f"âŒ æœªçŸ¥æƒ…ç·’: {args.emotion}")
        print(f"å¯ç”¨æƒ…ç·’: {', '.join(EMOTION_PROMPTS.keys())}")
        sys.exit(1)
    
    # è™•ç†é è¨­è²éŸ³
    if args.voice:
        if args.voice not in PRESET_VOICES:
            print(f"âŒ æœªçŸ¥è²éŸ³: {args.voice}")
            print(f"å¯ç”¨è²éŸ³: {', '.join(PRESET_VOICES.keys())}")
            sys.exit(1)
        
        voice_info = PRESET_VOICES[args.voice]
        ref_audio = find_voice_file(voice_info["audio"])
        
        # æ ¹æ“šæƒ…ç·’é¸æ“‡å°æ‡‰çš„åƒè€ƒæ–‡æœ¬
        ref_text = get_emotion_text(args.voice, args.emotion, voice_info["text"])
    else:
        if not args.ref_audio or not args.ref_text:
            print("âŒ è«‹æä¾› --ref_audio å’Œ --ref_textï¼Œæˆ–ä½¿ç”¨ --voice é¸æ“‡é è¨­è²éŸ³")
            sys.exit(1)
        ref_audio = args.ref_audio
        ref_text = args.ref_text
    
    # ç”Ÿæˆ
    try:
        output_file = generate_voice(
            text=args.text,
            ref_audio=ref_audio,
            ref_text=ref_text,
            output_path=args.output,
            model_id=args.model,
            emotion=args.emotion,
            verbose=not args.quiet
        )
        
        if args.quiet:
            print(output_file)
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
